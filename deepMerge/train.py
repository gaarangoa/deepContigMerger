from deepMerge import nt2vec
import click
import os
import multiprocessing

from deepMerge.parse import chunks

@click.command()
@click.option('--input-dir', default='', help='tabular file where the first field is the fasta file and the second field the label of the fasta file')
@click.option('--output-file', default='', help='output file to write the model')
@click.option('--training-chunk', default=100, help='Number of genomes used to train the model iteratively (default 1000).')
@click.option('--embedding-size', default=100, help='Length of embedding vector size (100 default)')
@click.option('--epochs', default=10, help='Number of training epochs (10 default)')
@click.option('--cores', help='Number of training epochs')
@click.option('--minp', help='minimum position training')
@click.option('--maxp', help='maximum position in training vector')
def train(input_dir, output_file, training_chunk, embedding_size, epochs, cores, minp, maxp):

    '''
        Train a word vector model.

        This script trains a sentence vector model. The input files are the h5 files generated by index.

        Because large corpus would take long to compute the word vectors, we implemented a training-chunk of the input files. This means that the
        model will be created and updated iteratively. This approach can be risky as it can vanish the already computed weights. need more exploration.

        The output is the gensim model that can be used to compute the word vectors of any input sequence.
     '''

    if not input_dir:
        print('\nUsage: No input file, type --help\n')
        exit()

    if not cores:
        cores = multiprocessing.cpu_count()
        print('num of cores is %s' % cores)

    _genomes = os.listdir(input_dir)
    _genomes_list = chunks(_genomes, training_chunk)


    for ix, _genome_chunk in enumerate(_genomes_list):
        print("processing chunk ", ix)
        print('number of genomes in chunk: ', len(_genome_chunk))

        _genomes_subset = {
            'doc_list': _genome_chunk,
            'doc_dir': input_dir
        }

        nt2vec.build(
            genome_list=_genomes_subset,
            model_filename=output_file,
            max_epochs=epochs,
            vec_size=embedding_size,
            cores=cores
        )
